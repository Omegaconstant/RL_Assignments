{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS6700 : Programming Assignment - 3\n",
    "Students:\n",
    "- Janmenjaya Panda (ME20B087)\n",
    "- Nishant Sahoo (ME20B122)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import os\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "import random\n",
    "import time\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('plots', exist_ok=True)\n",
    "os.makedirs('results', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9 # Discount Factor\n",
    "\n",
    "# Defining the Environment\n",
    "env = gym.make(\"Taxi-v3\", render_mode=\"rgb_array\")\n",
    "# render modes : human, rgb_array, ansi\n",
    "env.reset()\n",
    "# env.render()\n",
    "\n",
    "print(\"Current State : \", env.s)\n",
    "\n",
    "# Print number of states and actions\n",
    "print(\"Number of States : \", env.observation_space.n)\n",
    "print(\"Number of Actions : \", env.action_space.n)\n",
    "\n",
    "actions = {0 : \"South\", 1 : \"North\", 2 : \"East\", 3 : \"West\", 4 : \"Pickup\", 5 : \"Dropoff\"}\n",
    "\n",
    "next_state, reward, done, info = env.step(1)\n",
    "print(\"Action : \", actions[1])\n",
    "print(\"Next State : \", next_state)\n",
    "print(\"Reward : \", reward)\n",
    "print(\"Done : \", done)\n",
    "print(\"Info : \", info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_state(state):\n",
    "    row, col, _, _ = env.decode(state)\n",
    "    return [row, col]\n",
    "\n",
    "dest_loc = {\"R\": [0,0], \"G\": [0,4], \"Y\": [4,0], \"B\": [4,3]}\n",
    "loc_idx = {\"R\": 0, \"G\": 1, \"Y\": 2, \"B\": 3, \"in_taxi\": 4}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def egreedy_policy(Q, state, epsilon):\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        return np.argmax(Q[state, :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Option:\n",
    "    '''\n",
    "    An option is tuple of <I, pi, beta> where\n",
    "    I: initiation set\n",
    "    pi: policy\n",
    "    beta: termination set\n",
    "    '''\n",
    "    def __init__(self, initiation_set, actions, termination_set, name):\n",
    "        self.initiation_set = initiation_set  # States where the option can be initiated\n",
    "        self.actions = actions  # Actions that the option can take\n",
    "        self.termination_set = termination_set  # States where the option terminates\n",
    "        self.name = name  # Name of the option for identification\n",
    "        self.q_values = np.zeros((500, 4)) # Q-values for each state and primitive action\n",
    "        self.optdone = False\n",
    "\n",
    "    def act(self, state, epsilon = 0.1): # not needed actually (can use epsilon greedy policy function instead)\n",
    "        if np.random.rand() < epsilon:\n",
    "            return np.random.choice(self.actions)\n",
    "        else:\n",
    "            return np.argmax(self.q_values[state])\n",
    "        \n",
    "    def done(self, state):\n",
    "        optdone = state in self.termination_set\n",
    "        return optdone\n",
    "    \n",
    "\n",
    "# Define the initiation and termination sets for each option\n",
    "termination_sets = [set() for _ in range(4)] # R, G, Y, B\n",
    "\n",
    "for state in range(500):\n",
    "    if decode_state(state) == dest_loc[\"R\"]: # If the taxi is at the red location\n",
    "        termination_sets[0].add(state) \n",
    "    if decode_state(state) == dest_loc[\"G\"]:\n",
    "        termination_sets[1].add(state)\n",
    "    if decode_state(state) == dest_loc[\"Y\"]:\n",
    "        termination_sets[2].add(state)\n",
    "    if decode_state(state) == dest_loc[\"B\"]:\n",
    "        termination_sets[3].add(state)\n",
    "\n",
    "initiation_sets = [set(range(500)) - term_set for term_set in termination_sets]\n",
    "\n",
    "# Define the options\n",
    "options = [] # List of options: Move_to_R, Move_to_G, Move_to_Y, Move_to_B\n",
    "\n",
    "for loc, idx in loc_idx.items():\n",
    "    if loc != \"in_taxi\":  # Skip the \"in_taxi\" location\n",
    "        initiation_set = initiation_sets[idx]\n",
    "        termination_set = termination_sets[idx]\n",
    "        option_name = f\"Move_to_{loc}\"\n",
    "        option = Option(initiation_set, list(range(4)), termination_set, option_name)\n",
    "        options.append(option)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(options[0].termination_set)\n",
    "print(options[1].termination_set)\n",
    "print(options[2].termination_set)\n",
    "print(options[3].termination_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing SMDP Q-Learning\n",
    "\n",
    "def smdp_q_learning(env, options, num_episodes=1000, alpha=0.1, epsilon=0.001):\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    rewards = []\n",
    "    for episode in tqdm(range(num_episodes)):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            eps = max(0.98**episode, 1e-6)\n",
    "            action = egreedy_policy(Q, state, eps)\n",
    "            \n",
    "            # if primitive action\n",
    "            if action < env.action_space.n:\n",
    "                # Perform regular Q-Learning update\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                Q[state][action] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[state][action])\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "            else: # if option\n",
    "                option = options[action - env.action_space.n]\n",
    "                initial_state = state\n",
    "                option_reward = 0\n",
    "                discount = 1\n",
    "                steps = 0\n",
    "                while not option.done(state) and steps < 100:\n",
    "                    optact = option.act(state)\n",
    "                    next_state, reward, done, _ = env.step(optact)\n",
    "                    option_reward += discount * reward\n",
    "                    discount *= gamma\n",
    "                    steps += 1\n",
    "                    temp_reward = option_reward\n",
    "                    if option.done(next_state):\n",
    "                        temp_reward += 20 # Reward for completing the option\n",
    "                    # Update the Q-value within the option\n",
    "                    option.q_values[state][optact] += alpha * (temp_reward + gamma * np.max(option.q_values[next_state]) - option.q_values[state][optact])\n",
    "                    state = next_state\n",
    "\n",
    "                # Update the Q-value of the option\n",
    "                Q[initial_state][action] += alpha * (option_reward + discount * np.max(Q[state]) - Q[initial_state][action])\n",
    "                total_reward += option_reward\n",
    "        rewards.append(total_reward)\n",
    "    return Q, rewards\n",
    "\n",
    "# Training the SMDP Q-Learning Agent\n",
    "Q, rewards = smdp_q_learning(env, options, num_episodes=1500, alpha=0.1, epsilon=0.001)\n",
    "# Save the Q-values and rewards\n",
    "np.save(\"results/smdp_q_learning.npy\", Q)\n",
    "np.save(\"results/smdp_q_learning_rewards.npy\", rewards)\n",
    "\n",
    "# Plotting the Rewards\n",
    "plt.plot(rewards)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.title(\"SMDP Q-Learning Rewards\")\n",
    "plt.show()\n",
    "\n",
    "# # Visualizing the Q-Values\n",
    "# plt.figure(figsize=(15, 10))\n",
    "# for i in range(6):\n",
    "#     plt.subplot(2, 3, i + 1)\n",
    "#     sns.heatmap(Q[:, i].reshape(25, 20), cmap=\"coolwarm\", cbar=False)\n",
    "#     plt.title(f\"Action {actions[i]}\")\n",
    "# plt.suptitle(\"SMDP Q-Learning Q-Values\")\n",
    "# plt.tight_layout()\n",
    "# plt.subplots_adjust(top=0.9)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing Intra-Option Q-Learning\n",
    "# Logic:\n",
    "# At every step, the state-action value for the \n",
    "# primitive action as well as the state-action value \n",
    "# for all options that would have selected the same \n",
    "# action are updated, regardless of the option in \n",
    "# effect.\n",
    "\n",
    "# Let pie_0 be : a1 (for s1), a2 (for s2), a3 (for s3), ...\n",
    "#  for options:\n",
    "# Q(s1, o) = Q(s1, o) + alpha * (r1 + gamma * Q(s2, o) - Q(s1, o)) -> if o does not terminate in s2\n",
    "# Q(s1, o) = Q(s1, o) + alpha * (r1 + gamma * max_a (Q(s2, a)) - Q(s1, o)) -> if o terminates in s2\n",
    "\n",
    "def intra_option_q_learning(env, options, num_episodes=1000, alpha=0.1, epsilon=0.1):\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n + len(options)))\n",
    "    rewards = []\n",
    "    for episode in tqdm(range(num_episodes)):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            eps = max(0.98**episode, 1e-6)\n",
    "            action = egreedy_policy(Q, state, eps)\n",
    "            \n",
    "            # if primitive action\n",
    "            if action < env.action_space.n:\n",
    "                # Perform regular Q-Learning update\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                Q[state][action] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[state][action])\n",
    "                total_reward += reward\n",
    "            \n",
    "            # Update state-action values of options that would have selected the same action\n",
    "            for option in options:\n",
    "                if action in option.actions:\n",
    "                    next_state, reward, done, _ = env.step(option.act(state))\n",
    "                    if option.done(next_state): # if option terminates in next state\n",
    "                        Q[state][env.action_space.n + options.index(option)] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[state][env.action_space.n + options.index(option)])\n",
    "                    else: # if option does not terminate in next state\n",
    "                        Q[state][env.action_space.n + options.index(option)] += alpha * (reward + gamma * Q[next_state][env.action_space.n + options.index(option)] - Q[state][env.action_space.n + options.index(option)])\n",
    "                        \n",
    "                    # Update the Q-value within the option\n",
    "                    option.q_values[state][action] += alpha * (reward + gamma * np.max(option.q_values[next_state]) - option.q_values[state][action])\n",
    "                    \n",
    "            state = next_state\n",
    "        rewards.append(total_reward)\n",
    "    return Q, rewards\n",
    "\n",
    "# Training the Intra-Option Q-Learning Agent\n",
    "Q, rewards = intra_option_q_learning(env, options, num_episodes=1500, alpha=0.1, epsilon=0.1)\n",
    "# Save the Q-values and rewards\n",
    "np.save(\"results/intra_option_q_learning.npy\", Q)\n",
    "np.save(\"results/intra_option_q_learning_rewards.npy\", rewards)\n",
    "\n",
    "# Plotting the Rewards\n",
    "plt.plot(rewards)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.title(\"Intra-Option Q-Learning Rewards\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
