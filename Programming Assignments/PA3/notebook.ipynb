{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nN6WduLmYHyT"
      },
      "source": [
        "# CS6700 : Programming Assignment - 3\n",
        "Students:\n",
        "- Janmenjaya Panda (ME20B087)\n",
        "- Nishant Sahoo (ME20B122)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TLR-vPxIYHyc"
      },
      "outputs": [],
      "source": [
        "# 1 Hierarchical Reinforcement Learning\n",
        "# For this assignment, we will be referring to Sutton, Precup and Singh’s 1999 paper, ‘Between MDPs and semi-MDPs : A Framework for Temporal Abstraction in Reinforcement\n",
        "# good reference leading up to the understanding and implementation of SMDP Q-learning.\n",
        "# Section 3 of the paper talks about SMDP planning and is necessary to build intuition to\n",
        "# solve this assignment. We will be working with a simple taxi domain environment (explained\n",
        "# in the next section). Your tasks are to implement 1-step SMDP Q-Learning and intraoption Q-Learning on this environment.\n",
        "# 2 Environment Description\n",
        "# The environment for this task is the taxi domain, illustrated in Fig. 1. It is a 5x5 matrix,\n",
        "# where each cell is a position your taxi can stay at. There is a single passenger who can\n",
        "# be either picked up or dropped off, or is being transported. There are four designated\n",
        "# locations in the grid world indicated by R(ed), G(reen), Y(ellow), and B(lue). When the\n",
        "# episode starts, the taxi starts off at a random square and the passenger is at a random\n",
        "# location. The taxi drives to the passenger’s location, picks up the passenger, drives to the\n",
        "# passenger’s destination (another one of the four specified locations), and then drops off the\n",
        "# passenger. Once the passenger is dropped off, the episode ends.\n",
        "# There are 500 discrete states since there are 25 taxi positions, 5 possible locations of the\n",
        "# passenger (including the case when the passenger is in the taxi), and 4 destination locations.\n",
        "# Note that there are 400 states that can actually be reached during an episode. The missing\n",
        "# states correspond to situations in which the passenger is at the same location as their\n",
        "# destination, as this typically signals the end of an episode. Four additional states can be\n",
        "# observed right after a successful episodes, when both the passenger and the taxi are at the\n",
        "# destination. This gives a total of 404 reachable discrete states.\n",
        "# Passenger locations: 0: R(ed); 1: G(reen); 2: Y(ellow); 3: B(lue); 4: in taxi\n",
        "# Destinations: 0: R(ed); 1: G(reen); 2: Y(ellow); 3: B(lue)\n",
        "# Rewards:\n",
        "# • -1 per step unless other reward is triggered.\n",
        "# • +20 delivering passenger.\n",
        "# • -10 executing ”pickup” and ”drop-off” actions illegally.\n",
        "# The discount factor is taken to be γ = 0.9.\n",
        "# 1\n",
        "# Learning’. Please read the paper upto and including Section 3, it is self explanatory and a\n",
        "# Programming Assignment 3\n",
        "# April 6, 2024\n",
        "# CS6700: Reinforcement Learning\n",
        "# Figure 1: Taxi Domain\n",
        "# 3 Actions and Options\n",
        "# Actions: There are 6 discrete deterministic actions: 0: move south; 1: move north; 2:\n",
        "# move east; 3: move west; 4: pick passenger up; and 5: drop passenger off.\n",
        "# Options: Options to move the taxi to each of the four designated locations, executable\n",
        "# when the taxi is not already there.\n",
        "# 4 Tasks\n",
        "# First, implement the single step SMDP Q-learning for solving the taxi problem. A rough\n",
        "# sketch of the algorithm is as follows: Given the set of options,\n",
        "# • Execute the current selected option to termination (e.g. use epsilon greedy Q(s, o)).\n",
        "# • Computer r(s, o).\n",
        "# • Update Q(st\n",
        "# , o).\n",
        "# Second, implement intra-option Q-Learning on the same environment.\n",
        "# For each algorithm, do the following (only for the configuration with the best hyperparameters):\n",
        "# 1. Plot reward curves and visualize the learned Q-values.\n",
        "# 2. Provide a written description of the policies learnt and your reasoning behind why\n",
        "# the respective algorithm learns the policy.\n",
        "# 2\n",
        "# You will be experimenting with Gymnasium Gym’s Taxi-v3 environment.\n",
        "# 3. Is there an alternate set of options that you can use to solve this problem, such that\n",
        "# this set and the given options to move the taxi are mutually exclusive? If so, run\n",
        "# both algorithms with this alternate set of options and compare performance with the\n",
        "# algorithms run on the options to move the taxi.\n",
        "# Finally, provide a comparison between the SMDP Q-Learning and intra-option Q-Learning\n",
        "# algorithms. Do you observe any improvement with intra-option Q-Learning? If so, describe\n",
        "# why this happens as well. Please make sure that all descriptions are brief and to the point.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NX4kebG_YHyh"
      },
      "outputs": [],
      "source": [
        "# Importing Libraries\n",
        "import os\n",
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from IPython.display import clear_output\n",
        "import random\n",
        "import time\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KIh1AJpDYHyk"
      },
      "outputs": [],
      "source": [
        "os.makedirs('plots', exist_ok=True)\n",
        "os.makedirs('results', exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cEsCdIeYHyl",
        "outputId": "63497c1b-7ca7-4d1e-b47a-07acfa061daf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current State :  154\n",
            "Number of States :  500\n",
            "Number of Actions :  6\n",
            "Action :  North\n",
            "Next State :  54\n",
            "Reward :  -1\n",
            "Done :  False\n",
            "Info :  {'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)}\n"
          ]
        }
      ],
      "source": [
        "gamma = 0.9 # Discount Factor\n",
        "\n",
        "# Defining the Environment\n",
        "# env = gym.make(\"Taxi-v3\", render_mode=\"rgb_array\")\n",
        "env = gym.make(\"Taxi-v3\")\n",
        "# render modes : human, rgb_array, ansi\n",
        "env.reset()\n",
        "# env.render()\n",
        "\n",
        "print(\"Current State : \", env.s)\n",
        "\n",
        "# Print number of states and actions\n",
        "print(\"Number of States : \", env.observation_space.n)\n",
        "print(\"Number of Actions : \", env.action_space.n)\n",
        "\n",
        "actions = {0 : \"South\", 1 : \"North\", 2 : \"East\", 3 : \"West\", 4 : \"Pickup\", 5 : \"Dropoff\"}\n",
        "\n",
        "next_state, reward, done, info = env.step(1)\n",
        "print(\"Action : \", actions[1])\n",
        "print(\"Next State : \", next_state)\n",
        "print(\"Reward : \", reward)\n",
        "print(\"Done : \", done)\n",
        "print(\"Info : \", info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "i5XuzG0dYHym"
      },
      "outputs": [],
      "source": [
        "def decode_state(state):\n",
        "    row, col, _, _ = env.decode(state)\n",
        "    return row, col\n",
        "\n",
        "dest_loc = {\"R\": [0,0], \"G\": [0,4], \"Y\": [4,0], \"B\": [4,3]}\n",
        "loc_idx = {\"R\": 0, \"G\": 1, \"Y\": 2, \"B\": 3, \"in_taxi\": 4}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qgXwunr_YHyn"
      },
      "outputs": [],
      "source": [
        "def plot_Q(Q, title):\n",
        "        optimal_action_grid = np.zeros((5, 5)) # stores the optimal action taken for each cell for all the states that have cell as taxi position\n",
        "        # eg: [0,0] has states from 0-20 ; [0,1] has states from 21-40 and so on\n",
        "        Q_grid = np.ones((5, 5)) * -1000 # stores the max Q value for each cell for all the states that have cell as taxi position\n",
        "        state = 0\n",
        "        while state < 500:\n",
        "            row, col = decode_state(state)\n",
        "            Q_grid[row, col] = np.max(Q[state])\n",
        "            optimal_action_grid[row, col] = np.argmax(Q[state])\n",
        "            for i in range(20):\n",
        "                if Q_grid[row, col] < np.max(Q[state + i]):\n",
        "                    Q_grid[row, col] = np.max(Q[state + i])\n",
        "                    optimal_action_grid[row, col] = np.argmax(Q[state + i])\n",
        "            state += 20\n",
        "\n",
        "        plt.figure(figsize=(10, 10))\n",
        "        plt.imshow(Q_grid, cmap='coolwarm')\n",
        "\n",
        "        # Add annotations\n",
        "        for i in range(5):\n",
        "            for j in range(5):\n",
        "                action = optimal_action_grid[i, j]\n",
        "                if action == 0: # put down arrow\n",
        "                    plt.text(j, i, u'\\u2193', color='black', fontsize=20, ha='center', va='center')\n",
        "                elif action == 1: # put up arrow\n",
        "                    plt.text(j, i, u'\\u2191', color='black', fontsize=20, ha='center', va='center')\n",
        "                elif action == 2: # put right arrow\n",
        "                    plt.text(j, i, u'\\u2192', color='black', fontsize=20, ha='center', va='center')\n",
        "                elif action == 3: # put left arrow\n",
        "                    plt.text(j, i, u'\\u2190', color='black', fontsize=20, ha='center', va='center')\n",
        "                elif action == 4: # put Pick\n",
        "                    plt.text(j, i, \"Pick\", color='black', fontsize=20, ha='center', va='center')\n",
        "                elif action == 5: # put Drop\n",
        "                    plt.text(j, i, \"Drop\", color='black', fontsize=20, ha='center', va='center')\n",
        "\n",
        "        plt.axhline(y=-0.5, color='black', linewidth=3)\n",
        "        plt.axhline(y=4.5, color='black', linewidth=3)\n",
        "        plt.axvline(x=-0.5, color='black', linewidth=3)\n",
        "        plt.axvline(x=4.5, color='black', linewidth=3)\n",
        "\n",
        "        # Add horizontal grid lines\n",
        "        for i in range(6):\n",
        "            plt.axhline(y=i-0.5, color='black', linewidth=1)\n",
        "\n",
        "        # Add vertical grid lines\n",
        "        for i in range(6):\n",
        "            plt.axvline(x=i-0.5, color='black', linewidth=1)\n",
        "\n",
        "        plt.axvline(x=1.5, ymin=0.8, ymax=1, color='black', linewidth=3)\n",
        "        plt.axvline(x=0.5, ymin=0, ymax=0.4, color='black', linewidth=3)\n",
        "        plt.axvline(x=2.5, ymin=0, ymax=0.4, color='black', linewidth=3)\n",
        "\n",
        "        plt.colorbar()\n",
        "        plt.title(title)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kojnS7rBYHyp"
      },
      "outputs": [],
      "source": [
        "class Plot():\n",
        "    def __init__(self, env, rewards, Q, name):\n",
        "        self.env = env\n",
        "        self.rewards = rewards\n",
        "        self.Q = Q\n",
        "        self.name = name\n",
        "\n",
        "    def visualize_taxi_env(self):\n",
        "        grid = np.ones((5, 5, 3)) # 5x5 grid with 3 channels for RGB\n",
        "\n",
        "        taxi_row, taxi_col, pass_idx, dest_idx = self.env.decode(self.env.s)\n",
        "\n",
        "        grid[0,0] = [1, 0, 0] # Red\n",
        "        grid[0,4] = [0, 1, 0] # Green\n",
        "        grid[4,0] = [1, 1, 0] # Yellow\n",
        "        grid[4,3] = [0, 0, 1] # Blue\n",
        "\n",
        "        plt.figure(figsize=(10, 10))\n",
        "        plt.imshow(grid)\n",
        "\n",
        "        # Add horizontal grid lines\n",
        "        for i in range(6):\n",
        "            plt.axhline(y=i-0.5, color='black', linewidth=1)\n",
        "\n",
        "        # Add vertical grid lines\n",
        "        for i in range(6):\n",
        "            plt.axvline(x=i-0.5, color='black', linewidth=1)\n",
        "\n",
        "        # Add taxi\n",
        "        plt.text(taxi_col, taxi_row, \"T\", color=\"black\", fontsize=20, ha=\"center\", va=\"center\")\n",
        "        # plt.plot(taxi_col, taxi_row, marker='s', markersize=20, color='black')\n",
        "\n",
        "        # # Add passenger\n",
        "        pass_states = [(0, 0), (0, 4), (4, 0), (4, 3), (taxi_row, taxi_col)]\n",
        "        if pass_idx < 4:\n",
        "            plt.text(pass_states[pass_idx][1], pass_states[pass_idx][0] - 0.3, \"P\", color=\"black\", fontsize=20, ha=\"center\", va=\"center\")\n",
        "            # plt.plot(pass_states[pass_idx][1], pass_states[pass_idx][0] - 0.3, marker='P', markersize=20, color='black')\n",
        "        else:\n",
        "            plt.text(taxi_col, taxi_row - 0.3, \"P\", color=\"black\", fontsize=20, ha=\"center\", va=\"center\")\n",
        "            # plt.plot(taxi_col, taxi_row - 0.3, marker='P', markersize=20, color='black')\n",
        "\n",
        "        # Add Destination\n",
        "        plt.text(dest_loc[\"R\"][1], dest_loc[\"R\"][0] + 0.3, \"D\", color=\"black\", fontsize=20, ha=\"center\", va=\"center\")\n",
        "        # plt.plot(dest_loc[\"R\"][1], dest_loc[\"R\"][0] + 0.3, marker='D', markersize=20, color='black')\n",
        "\n",
        "        # Add boundary\n",
        "        plt.axhline(y=-0.5, color='black', linewidth=3)\n",
        "        plt.axhline(y=4.5, color='black', linewidth=3)\n",
        "        plt.axvline(x=-0.5, color='black', linewidth=3)\n",
        "        plt.axvline(x=4.5, color='black', linewidth=3)\n",
        "\n",
        "        # +---------+\n",
        "        # |R: | : :G|\n",
        "        # | : | : : |\n",
        "        # | : : : : |\n",
        "        # | | : | : |\n",
        "        # |Y| : |B: |\n",
        "        # +---------+\n",
        "\n",
        "        # Add the walls\n",
        "        plt.axvline(x=1.5, ymin=0.8, ymax=1, color='black', linewidth=3)\n",
        "        plt.axvline(x=0.5, ymin=0, ymax=0.4, color='black', linewidth=3)\n",
        "        plt.axvline(x=2.5, ymin=0, ymax=0.4, color='black', linewidth=3)\n",
        "        plt.show()\n",
        "\n",
        "    def plot_Q(self):\n",
        "        plot_Q(self.Q, f\"{self.name} Q-values\")\n",
        "\n",
        "\n",
        "    def plot_rewards(self):\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.plot(self.rewards.mean(axis=0), color='blue')\n",
        "        plt.fill_between(range(self.rewards.shape[1]), self.rewards.mean(axis=0) - self.rewards.std(axis=0), self.rewards.mean(axis=0) + self.rewards.std(axis=0), alpha=0.3, color='blue')\n",
        "        plt.title(f\"{self.name} Rewards\")\n",
        "        plt.xlabel(\"Episodes\")\n",
        "        plt.ylabel(\"Reward\")\n",
        "        # plt.grid()\n",
        "        plt.savefig(f'plots/{self.name}_rewards.png')\n",
        "        plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qX3fqLytYHyr"
      },
      "outputs": [],
      "source": [
        "def egreedy_policy(Q, state, epsilon):\n",
        "    if random.uniform(0, 1) < epsilon:\n",
        "        return env.action_space.sample()\n",
        "    else:\n",
        "        return np.argmax(Q[state, :])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "OfKVW2BeYHyt"
      },
      "outputs": [],
      "source": [
        "class Option:\n",
        "    '''\n",
        "    An option is tuple of <I, pi, beta> where\n",
        "    I: initiation set\n",
        "    pi: policy\n",
        "    beta: termination set\n",
        "    '''\n",
        "    def __init__(self, initiation_set, actions, termination_set, name):\n",
        "        self.initiation_set = initiation_set  # States where the option can be initiated\n",
        "        self.actions = actions  # Actions that the option can take\n",
        "        self.termination_set = termination_set  # States where the option terminates\n",
        "        self.name = name  # Name of the option for identification\n",
        "        self.q_values = np.zeros((env.observation_space.n, len(actions))) # Q-values for each state and primitive action\n",
        "\n",
        "    def act(self, state, epsilon = 0.1): # not needed actually (can use epsilon greedy policy function instead)\n",
        "        if np.random.rand() < epsilon:\n",
        "            return np.random.choice(self.actions)\n",
        "        else:\n",
        "            return np.argmax(self.q_values[state])\n",
        "\n",
        "    def terminal(self, state):\n",
        "        return state in self.termination_set\n",
        "\n",
        "    def reset(self):\n",
        "        self.q_values = np.zeros((500, 4))\n",
        "\n",
        "\n",
        "# Define the initiation and termination sets for each option\n",
        "termination_sets = [set() for _ in range(4)] # R, G, Y, B\n",
        "\n",
        "for state in range(500):\n",
        "    if list(decode_state(state)) == dest_loc[\"R\"]: # If the taxi is at the red location\n",
        "        termination_sets[0].add(state)\n",
        "    if list(decode_state(state)) == dest_loc[\"G\"]:\n",
        "        termination_sets[1].add(state)\n",
        "    if list(decode_state(state)) == dest_loc[\"Y\"]:\n",
        "        termination_sets[2].add(state)\n",
        "    if list(decode_state(state)) == dest_loc[\"B\"]:\n",
        "        termination_sets[3].add(state)\n",
        "\n",
        "initiation_sets = [set(range(500)) - term_set for term_set in termination_sets]\n",
        "\n",
        "# Define the options\n",
        "options = [] # List of options: Move_to_R, Move_to_G, Move_to_Y, Move_to_B\n",
        "\n",
        "for loc, idx in loc_idx.items():\n",
        "    if loc != \"in_taxi\":  # Skip the \"in_taxi\" location\n",
        "        initiation_set = initiation_sets[idx]\n",
        "        termination_set = termination_sets[idx]\n",
        "        option_name = f\"Move_to_{loc}\"\n",
        "        option = Option(initiation_set, list(range(4)), termination_set, option_name)\n",
        "        options.append(option)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zWrJLowYHyu",
        "outputId": "84f2db22-27a2-4950-8d9a-845d23089207"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19}\n",
            "{80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99}\n",
            "{400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419}\n",
            "{460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479}\n"
          ]
        }
      ],
      "source": [
        "print(options[0].termination_set)\n",
        "print(options[1].termination_set)\n",
        "print(options[2].termination_set)\n",
        "print(options[3].termination_set)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "DdJH6mhfYHyv"
      },
      "outputs": [],
      "source": [
        "# for i, option in enumerate(options):\n",
        "#     plot_Q(option.q_values, f\"Q-values for {option.name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66o-c3sdYHyv"
      },
      "outputs": [],
      "source": [
        "# Implementing SMDP Q-Learning\n",
        "\n",
        "def smdp(env, options, num_episodes=1000, alpha=0.1, epsilon=0.001):\n",
        "    Q = np.zeros((env.observation_space.n, env.action_space.n + len(options)))\n",
        "    for opt in options:\n",
        "        opt.reset()\n",
        "    rewards = []\n",
        "    for episode in tqdm(range(num_episodes)):\n",
        "        state = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "        steps = 0\n",
        "        while not done and steps < 1000:\n",
        "            eps = max(0.98**steps, 1e-5)\n",
        "            steps += 1\n",
        "            action = egreedy_policy(Q, state, eps)\n",
        "\n",
        "            # if primitive action\n",
        "            if action < env.action_space.n:\n",
        "                # Perform regular Q-Learning update\n",
        "                next_state, reward, done, _ = env.step(action)\n",
        "                Q[state][action] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[state][action])\n",
        "                state = next_state\n",
        "                total_reward += reward\n",
        "            else: # if option\n",
        "                option = options[action - env.action_space.n]\n",
        "                initial_state = state\n",
        "                option_reward = 0\n",
        "                discount = 1\n",
        "                optsteps = 0\n",
        "                while not option.terminal(state):\n",
        "                    optact = option.act(state, eps)\n",
        "                    next_state, reward, done, _ = env.step(optact)\n",
        "                    option_reward += discount * reward\n",
        "                    discount *= gamma\n",
        "                    optsteps += 1\n",
        "                    # Update the Q-value within the option\n",
        "                    option.q_values[state][optact] += alpha * (reward + gamma * np.max(option.q_values[next_state]) - option.q_values[state][optact])\n",
        "                    state = next_state\n",
        "\n",
        "                # Update the Q-value of the option\n",
        "                Q[initial_state][action] += alpha * (option_reward + discount * np.max(Q[state]) - Q[initial_state][action])\n",
        "                total_reward += option_reward\n",
        "        rewards.append(total_reward)\n",
        "    return Q, rewards\n",
        "\n",
        "# Training the SMDP Q-Learning Agent\n",
        "Q, rewards = smdp(env, options, num_episodes=10000, alpha=0.1, epsilon=0.001)\n",
        "np.save(\"results/smdp_Q.npy\", Q)\n",
        "np.save(\"results/smdp_rewards.npy\", rewards)\n",
        "\n",
        "\n",
        "# Plotting the results\n",
        "plot = Plot(env, rewards, Q, \"smdp\")\n",
        "plot.plot_rewards()\n",
        "plot.plot_Q()\n",
        "plot.visualize_taxi_env()\n",
        "\n",
        "# Plot Q-Table of options\n",
        "for i, option in enumerate(options):\n",
        "    plot_Q(option.q_values, f\"Q-values for {option.name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ShwBLy1PYHyw"
      },
      "outputs": [],
      "source": [
        "# Implementing Intra-Option Q-Learning\n",
        "# Logic:\n",
        "# At every step, the state-action value for the\n",
        "# primitive action as well as the state-action value\n",
        "# for all options that would have selected the same\n",
        "# action are updated, regardless of the option in\n",
        "# effect.\n",
        "\n",
        "# Let pie_0 be : a1 (for s1), a2 (for s2), a3 (for s3), ...\n",
        "#  for options:\n",
        "# Q(s1, o) = Q(s1, o) + alpha * (r1 + gamma * Q(s2, o) - Q(s1, o)) -> if o does not terminate in s2\n",
        "# Q(s1, o) = Q(s1, o) + alpha * (r1 + gamma * max_a (Q(s2, a)) - Q(s1, o)) -> if o terminates in s2\n",
        "\n",
        "def intra_option(env, options, num_episodes=1000, alpha=0.1, epsilon=0.1):\n",
        "    Q = np.zeros((env.observation_space.n, env.action_space.n + len(options)))\n",
        "    for opt in options:\n",
        "        opt.reset()\n",
        "    rewards = []\n",
        "    for episode in tqdm(range(num_episodes)):\n",
        "        state = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "        steps = 0\n",
        "        while not done and steps < 1000:\n",
        "            action = egreedy_policy(Q, state, eps)\n",
        "            eps = max(0.98**steps, 1e-5)\n",
        "            steps += 1\n",
        "\n",
        "            # if primitive action\n",
        "            if action < env.action_space.n:\n",
        "                # Perform regular Q-Learning update\n",
        "                next_state, reward, done, _ = env.step(action)\n",
        "                Q[state][action] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[state][action])\n",
        "\n",
        "                # # Update state-action values of options that would have selected the same action\n",
        "                # for option in options: # R,G,Y,B\n",
        "                #     # if action in option.policy[state]:\n",
        "                #     if action in option.actions:\n",
        "                #         optidx = env.action_space.n + options.index(option)\n",
        "                #         Q[state][optidx] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[state][optidx])\n",
        "\n",
        "                #         # Update the Q-value within the option\n",
        "                #         option.q_values[state][action] += alpha * (reward + gamma * np.max(option.q_values[next_state]) - option.q_values[state][action])\n",
        "\n",
        "                # state = next_state\n",
        "\n",
        "                next_action = egreedy_policy(Q, next_state, eps)\n",
        "                for option in options:\n",
        "                        if action in option.actions:\n",
        "                            if option.terminal(next_state):\n",
        "                                Q[state][env.action_space.n + options.index(option)] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[state][env.action_space.n + options.index(option)])\n",
        "                                option.q_values[state][action] += alpha * (reward + gamma * np.max(option.q_values[next_state]) - option.q_values[state][action])\n",
        "                            else:\n",
        "                                Q[state][env.action_space.n + options.index(option)] += alpha * (reward + gamma * Q[next_state][next_action] - Q[state][env.action_space.n + options.index(option)])\n",
        "                                option.q_values[state][action] += alpha * (reward + gamma * option.q_values[next_state][next_action] - option.q_values[state][action])\n",
        "\n",
        "                state = next_state\n",
        "                action = next_action\n",
        "\n",
        "                total_reward += reward\n",
        "\n",
        "            else:\n",
        "                curr_option = options[action - env.action_space.n]\n",
        "                optsteps = 0\n",
        "                optact = curr_option.act(state, eps)\n",
        "                while not curr_option.terminal(state):\n",
        "                    next_state, reward, done, _ = env.step(optact)\n",
        "                    optsteps += 1\n",
        "\n",
        "                    # Update the Q-value of the primitive action\n",
        "                    Q[state][optact] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[state][optact])\n",
        "                    # Update the Q-value of all options that would have selected the same action\n",
        "                    next_action = curr_option.act(next_state, eps)\n",
        "                    for option in options:\n",
        "                        if optact in option.actions:\n",
        "                            if option.terminal(next_state):\n",
        "                                Q[state][env.action_space.n + options.index(option)] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[state][env.action_space.n + options.index(option)])\n",
        "                                option.q_values[state][optact] += alpha * (reward + gamma * np.max(option.q_values[next_state]) - option.q_values[state][optact])\n",
        "                            else:\n",
        "                                Q[state][env.action_space.n + options.index(option)] += alpha * (reward + gamma * Q[next_state][next_action] - Q[state][env.action_space.n + options.index(option)])\n",
        "                                option.q_values[state][optact] += alpha * (reward + gamma * option.q_values[next_state][next_action] - option.q_values[state][optact])\n",
        "\n",
        "                    state = next_state\n",
        "                    optact = next_action\n",
        "                total_reward += reward\n",
        "        rewards.append(total_reward)\n",
        "    return Q, rewards\n",
        "\n",
        "# Training the Intra-Option Q-Learning Agent\n",
        "Q, rewards = intra_option(env, options, num_episodes=1500, alpha=0.1, epsilon=0.1)\n",
        "# Save the Q-values and rewards\n",
        "np.save(\"results/intra_option.npy\", Q)\n",
        "np.save(\"results/intra_option_q_learning_rewards.npy\", rewards)\n",
        "\n",
        "# Plotting the Rewards\n",
        "plt.plot(rewards)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Total Reward\")\n",
        "plt.title(\"Intra-Option Q-Learning Rewards\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "6zSmRtesYHyw"
      },
      "outputs": [],
      "source": [
        "class Args:\n",
        "    def __init__(self):\n",
        "        self.num_episodes = 10000\n",
        "        self.max_steps = 1000\n",
        "        self.runs = 5\n",
        "\n",
        "        self.epsilon0 = 1.0\n",
        "        self.min_epsilon = 1e-5\n",
        "        self.epsilon_decay = 0.98\n",
        "        self.alpha = 0.1\n",
        "        self.option_alpha = 0.1\n",
        "        self.gamma = 0.9\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "asXExqfdYHyx"
      },
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "    def __init__(self, env, options, args, algo=\"smdp\"):\n",
        "        self.env = env\n",
        "        self.options = options\n",
        "        self.args = args\n",
        "        self.algo = algo\n",
        "\n",
        "    def train(self):\n",
        "        rewards = np.zeros((self.args.runs, self.args.num_episodes))\n",
        "        Qs = np.zeros((self.args.runs, self.env.observation_space.n, self.env.action_space.n + len(self.options)))\n",
        "        for run in range(self.args.runs):\n",
        "            if self.algo == \"smdp\":\n",
        "                Qs[run], rewards[run] = self.smdp()\n",
        "            # elif self.algo == \"intra_option\":\n",
        "            #     Qs[run], rewards[run] = intra_option(self.env, self.options, num_episodes=self.args.max_episodes, alpha=self.args.alpha, epsilon=self.args.epsilon0)\n",
        "        Q = np.mean(Qs, axis=0)\n",
        "        mean_rewards = np.mean(rewards, axis=0)\n",
        "        return Q, mean_rewards\n",
        "\n",
        "    # def smdp(env, options, num_episodes=1000, alpha=0.1, epsilon=0.001):\n",
        "    def smdp(self):\n",
        "        Q = np.zeros((self.env.observation_space.n, self.env.action_space.n + len(options)))\n",
        "        for opt in options:\n",
        "            opt.reset()\n",
        "        rewards = []\n",
        "        for episode in tqdm(range(self.args.num_episodes)):\n",
        "            state = self.env.reset()\n",
        "            total_reward = 0\n",
        "            done = False\n",
        "            steps = 0\n",
        "            while not done and steps < self.args.max_steps:\n",
        "                eps = max(0.98**steps, 1e-5)\n",
        "                steps += 1\n",
        "                action = egreedy_policy(Q, state, eps)\n",
        "\n",
        "                # if primitive action\n",
        "                if action < self.env.action_space.n:\n",
        "                    # Perform regular Q-Learning update\n",
        "                    next_state, reward, done, _ = self.env.step(action)\n",
        "                    Q[state][action] += self.args.alpha * (reward + self.args.gamma * np.max(Q[next_state]) - Q[state][action])\n",
        "                    state = next_state\n",
        "                    total_reward += reward\n",
        "                else: # if option\n",
        "                    option = options[action - self.env.action_space.n]\n",
        "                    initial_state = state\n",
        "                    option_reward = 0\n",
        "                    discount = 1\n",
        "                    optsteps = 0\n",
        "                    while not option.terminal(state):\n",
        "                        optact = option.act(state, eps)\n",
        "                        next_state, reward, done, _ = self.env.step(optact)\n",
        "                        option_reward += discount * reward\n",
        "                        discount *= self.args.gamma\n",
        "                        optsteps += 1\n",
        "                        # Update the Q-value within the option\n",
        "                        option.q_values[state][optact] += self.args.alpha * (reward + self.args.gamma * np.max(option.q_values[next_state]) - option.q_values[state][optact])\n",
        "                        state = next_state\n",
        "\n",
        "                    # Update the Q-value of the option\n",
        "                    Q[initial_state][action] += self.args.alpha * (option_reward + discount * np.max(Q[state]) - Q[initial_state][action])\n",
        "                    total_reward += option_reward\n",
        "            rewards.append(total_reward)\n",
        "        return Q, rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPk_f54KYHyy",
        "outputId": "e9d00b9a-92e9-43ca-8c9d-f15143a4c19e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [02:45<00:00, 60.40it/s]\n",
            " 67%|██████▋   | 6681/10000 [02:00<00:39, 84.48it/s]"
          ]
        }
      ],
      "source": [
        "# Implement SMDP Q-Learning\n",
        "args = Args()\n",
        "trainer = Trainer(env, options, args, algo=\"smdp\")\n",
        "Q, rewards = trainer.train()\n",
        "np.save(\"results/smdp_Q.npy\", Q)\n",
        "np.save(\"results/smdp_rewards.npy\", rewards)\n",
        "\n",
        "plot = Plot(env, rewards, Q, \"smdp\")\n",
        "plot.plot_rewards()\n",
        "plot.plot_Q()\n",
        "plot.visualize_taxi_env()\n",
        "\n",
        "# Plot Q-Table of options\n",
        "for i, option in enumerate(options):\n",
        "    plot_Q(option.q_values, f\"Q-values for {option.name}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}